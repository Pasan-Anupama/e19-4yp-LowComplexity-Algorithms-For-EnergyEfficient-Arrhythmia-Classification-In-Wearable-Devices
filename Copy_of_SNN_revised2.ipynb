{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mewanDimalsha/e19-4yp-LowComplexity-Algorithms-For-EnergyEfficient-Arrhythmia-Classification-In-Wearable-Devices/blob/main/Copy_of_SNN_revised2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pijagnYmap3T",
        "outputId": "6770ce29-d93e-47ae-a333-95c4af4a19e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Install necessary libraries\n",
        "!pip install wfdb neurokit2 imblearn snntorch torch numpy scipy matplotlib seaborn --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "uzI7fdoKa6V8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import wfdb\n",
        "\n",
        "# Define data directory in Google Drive\n",
        "data_dir = '/content/drive/MyDrive/ecg_snn_project/data/mitdb'\n",
        "if not os.path.exists(data_dir):\n",
        "    os.makedirs(data_dir)\n",
        "    wfdb.dl_database('mitdb', data_dir)  # Downloads MIT-BIH dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-lCaiVJnbht_"
      },
      "outputs": [],
      "source": [
        "# Function to load an ECG record\n",
        "def load_ecg(record_id, data_dir):\n",
        "    record = wfdb.rdrecord(f'{data_dir}/{record_id}')\n",
        "    annotation = wfdb.rdann(f'{data_dir}/{record_id}', 'atr')\n",
        "    signal = record.p_signal[:, 0]\n",
        "    fs = record.fs\n",
        "    return signal, annotation.sample, fs, annotation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "6f5AMeT3bmoV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy import signal\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Denoising\n",
        "def bandpass_filter(signal_arr, fs, lowcut=0.5, highcut=40):\n",
        "    nyquist = fs / 2\n",
        "    low = lowcut / nyquist\n",
        "    high = highcut / nyquist\n",
        "    b, a = signal.butter(2, [low, high], btype='band')\n",
        "    return signal.filtfilt(b, a, signal_arr)\n",
        "\n",
        "def notch_filter(signal_arr, fs, freq=50, Q=30):\n",
        "    b, a = signal.iirnotch(freq, Q, fs)\n",
        "    return signal.filtfilt(b, a, signal_arr)\n",
        "\n",
        "def remove_baseline(signal_arr, fs, window_size=0.2):\n",
        "    window_samples = int(window_size * fs)\n",
        "    baseline = signal.savgol_filter(signal_arr, window_samples, 2)\n",
        "    return signal_arr - baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "dYLdT58-grTB"
      },
      "outputs": [],
      "source": [
        "#segmentation\n",
        "def extract_heartbeats(signal_arr, fs, annotation_rpeaks, fixed_length=250):\n",
        "    beats = []\n",
        "    valid_rpeaks = []\n",
        "    half_length = fixed_length // 2\n",
        "    for rpeak in annotation_rpeaks:\n",
        "        start = rpeak - half_length\n",
        "        end = rpeak + half_length\n",
        "        if start >= 0 and end <= len(signal_arr):\n",
        "            beat = signal_arr[start:end]\n",
        "            beats.append(beat)\n",
        "            valid_rpeaks.append(rpeak)\n",
        "    return np.array(beats), np.array(valid_rpeaks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "qfiDv44YhISZ"
      },
      "outputs": [],
      "source": [
        "# Class Balancing\n",
        "def balance_classes(X, y):\n",
        "    smote = SMOTE(random_state=42)\n",
        "    X_balanced, y_balanced = smote.fit_resample(X, y)\n",
        "    return X_balanced, y_balanced"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "lJDaZAzYhLpY"
      },
      "outputs": [],
      "source": [
        "# Normalization\n",
        "def normalize_beats(beats):\n",
        "    min_val = beats.min(axis=1, keepdims=True)\n",
        "    max_val = beats.max(axis=1, keepdims=True)\n",
        "    return (beats - min_val) / (max_val - min_val + 1e-8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ropt-GPVhOK9"
      },
      "outputs": [],
      "source": [
        "# Label Creation\n",
        "AAMI_classes = {\n",
        "    0: ['N', 'L', 'R', 'e', 'j'],  # Normal\n",
        "    1: ['A', 'a', 'J', 'S', 'V', 'E', 'F', 'P', '/', 'f', 'u']  # Non-Normal\n",
        "}\n",
        "\n",
        "def get_class_from_symbol(symbol):\n",
        "    for class_id, symbols in AAMI_classes.items():\n",
        "        if symbol in symbols:\n",
        "            return class_id\n",
        "    return None\n",
        "\n",
        "def create_labels(rpeaks, annotation):\n",
        "    labels = []\n",
        "    for rpeak in rpeaks:\n",
        "        idx = np.where(annotation.sample == rpeak)[0]\n",
        "        if len(idx) > 0:\n",
        "            symbol = annotation.symbol[idx[0]]\n",
        "            label = get_class_from_symbol(symbol)\n",
        "            if label is not None:\n",
        "                labels.append(label)\n",
        "    return np.array(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "e7Lx26cehTBY"
      },
      "outputs": [],
      "source": [
        "# SNN definition\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import snntorch as snn\n",
        "from snntorch import surrogate\n",
        "class SNN(nn.Module):\n",
        "    def __init__(self, num_inputs=250, num_hidden=128, num_outputs=2, num_steps=25, beta=0.9):\n",
        "        super().__init__()\n",
        "        self.num_steps = num_steps\n",
        "        spike_grad = surrogate.fast_sigmoid(slope=25)\n",
        "\n",
        "        self.conv1 = nn.Conv1d(1, 16, kernel_size=5, stride=1, padding=2)\n",
        "        self.lif1 = snn.Leaky(beta=beta, spike_grad=spike_grad)\n",
        "        self.pool1 = nn.MaxPool1d(kernel_size=2)\n",
        "\n",
        "        self.conv2 = nn.Conv1d(16, 32, kernel_size=5, stride=1, padding=2)\n",
        "        self.lif2 = snn.Leaky(beta=beta, spike_grad=spike_grad)\n",
        "        self.pool2 = nn.MaxPool1d(kernel_size=2)\n",
        "\n",
        "        self.fc1 = nn.Linear(32 * (num_inputs // 4), num_hidden)\n",
        "        self.lif3 = snn.Leaky(beta=beta, spike_grad=spike_grad)\n",
        "\n",
        "        self.fc2 = nn.Linear(num_hidden, num_outputs)\n",
        "        self.lif4 = snn.Leaky(beta=beta, spike_grad=spike_grad)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        mem1 = self.lif1.init_leaky()\n",
        "        mem2 = self.lif2.init_leaky()\n",
        "        mem3 = self.lif3.init_leaky()\n",
        "        mem4 = self.lif4.init_leaky()\n",
        "\n",
        "        spk_rec = []\n",
        "        for _ in range(self.num_steps):\n",
        "            cur = self.conv1(x)\n",
        "            spk1, mem1 = self.lif1(self.pool1(cur), mem1)\n",
        "            cur = self.conv2(spk1)\n",
        "            spk2, mem2 = self.lif2(self.pool2(cur), mem2)\n",
        "            cur = spk2.view(batch_size, -1)\n",
        "            cur = self.fc1(cur)\n",
        "            spk3, mem3 = self.lif3(cur, mem3)\n",
        "            cur = self.fc2(spk3)\n",
        "            spk4, mem4 = self.lif4(cur, mem4)\n",
        "            spk_rec.append(spk4)\n",
        "\n",
        "        return torch.stack(spk_rec, dim=0).sum(dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Dn59-yWFhZPL"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix # Import the required metrics\n",
        "import os\n",
        "import seaborn as sns\n",
        "\n",
        "# Training\n",
        "def train_model(X_train, y_train, X_val, y_val, X_test, y_test, batch_size=64, num_epochs=10, device='cuda'):\n",
        "    model = SNN(num_inputs=X_train.shape[1]).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    X_train_tensor = torch.FloatTensor(X_train).permute(0, 2, 1).to(device)\n",
        "    y_train_tensor = torch.LongTensor(y_train).to(device)\n",
        "    X_val_tensor = torch.FloatTensor(X_val).permute(0, 2, 1).to(device)\n",
        "    y_val_tensor = torch.LongTensor(y_val).to(device)\n",
        "    X_test_tensor = torch.FloatTensor(X_test).permute(0, 2, 1).to(device)\n",
        "    y_test_tensor = torch.LongTensor(y_test).to(device)\n",
        "\n",
        "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    history = {\n",
        "        'train_loss': [], 'train_acc': [],\n",
        "        'val_loss': [], 'val_acc': [],\n",
        "        'test_loss': [], 'test_acc': []\n",
        "    }\n",
        "    checkpoint_dir = 'checkpoints'\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    checkpoint_paths = []\n",
        "\n",
        "    # Training phase: Train for all epochs and save checkpoints\n",
        "    print(\"Training phase...\")\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_train_loss = 0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_train_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_train += labels.size(0)\n",
        "            correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "        train_loss = running_train_loss / total_train\n",
        "        train_acc = correct_train / total_train\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "\n",
        "        # Save model checkpoint\n",
        "        checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch+1}.pt')\n",
        "        torch.save(model.state_dict(), checkpoint_path)\n",
        "        checkpoint_paths.append(checkpoint_path)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
        "\n",
        "    # Plot training metrics\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.plot(history['train_loss'], label='Train Loss', color='#1f77b4')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.plot(history['train_acc'], label='Train Accuracy', color='#1f77b4')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Training Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('training_metrics_partial.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Validation phase: Evaluate each checkpoint on the validation set\n",
        "    print(\"\\nEvaluating saved models on validation set...\")\n",
        "    for epoch, checkpoint_path in enumerate(checkpoint_paths):\n",
        "        model.load_state_dict(torch.load(checkpoint_path))\n",
        "        model.eval()\n",
        "        running_val_loss = 0\n",
        "        correct_val = 0\n",
        "        total_val = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                running_val_loss += loss.item() * inputs.size(0)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total_val += labels.size(0)\n",
        "                correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "        val_loss = running_val_loss / total_val\n",
        "        val_acc = correct_val / total_val\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "        print(f\"Validation Epoch {epoch+1}/{num_epochs}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # Test phase: Evaluate each checkpoint on the test set\n",
        "    print(\"\\nEvaluating saved models on test set...\")\n",
        "    for epoch, checkpoint_path in enumerate(checkpoint_paths):\n",
        "        model.load_state_dict(torch.load(checkpoint_path))\n",
        "        model.eval()\n",
        "        running_test_loss = 0\n",
        "        correct_test = 0\n",
        "        total_test = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in test_loader:\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                running_test_loss += loss.item() * inputs.size(0)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total_test += labels.size(0)\n",
        "                correct_test += (predicted == labels).sum().item()\n",
        "\n",
        "        test_loss = running_test_loss / total_test\n",
        "        test_acc = correct_test / total_test\n",
        "        history['test_loss'].append(test_loss)\n",
        "        history['test_acc'].append(test_acc)\n",
        "        print(f\"Test Epoch {epoch+1}/{num_epochs}, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")\n",
        "\n",
        "    # Plot combined metrics\n",
        "    plot_metrics(history)\n",
        "\n",
        "    # Clean up checkpoints\n",
        "    for checkpoint_path in checkpoint_paths:\n",
        "        os.remove(checkpoint_path)\n",
        "    os.rmdir(checkpoint_dir)\n",
        "\n",
        "    return model, history\n",
        "\n",
        "def evaluate_model(model, X_val, y_val, X_test, y_test, device='cuda'):\n",
        "    def compute_metrics(X, y, dataset_name):\n",
        "        X_tensor = torch.FloatTensor(X).permute(0, 2, 1).to(device)\n",
        "        y_tensor = torch.LongTensor(y).to(device)\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            outputs = model(X_tensor)\n",
        "            loss = nn.CrossEntropyLoss()(outputs, y_tensor).item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            y_np = y_tensor.cpu().numpy()\n",
        "            predicted_np = predicted.cpu().numpy()\n",
        "\n",
        "            accuracy = (predicted == y_tensor).float().mean().item()\n",
        "            precision = precision_score(y_np, predicted_np, average='binary')\n",
        "            recall = recall_score(y_np, predicted_np, average='binary')\n",
        "            f1 = f1_score(y_np, predicted_np, average='binary')\n",
        "            cm = confusion_matrix(y_np, predicted_np)\n",
        "\n",
        "            print(f\"\\n{dataset_name} Metrics:\")\n",
        "            print(f\"  Loss: {loss:.4f}\")\n",
        "            print(f\"  Accuracy: {accuracy:.4f}\")\n",
        "            print(f\"  Precision: {precision:.4f}\")\n",
        "            print(f\"  Recall: {recall:.4f}\")\n",
        "            print(f\"  F1-Score: {f1:.4f}\")\n",
        "            print(f\"  Confusion Matrix:\")\n",
        "            print(f\"    True Negative (Normal correct): {cm[0,0]}\")\n",
        "            print(f\"    False Positive (Normal as Non-Normal): {cm[0,1]}\")\n",
        "            print(f\"    False Negative (Non-Normal as Normal): {cm[1,0]}\")\n",
        "            print(f\"    True Positive (Non-Normal correct): {cm[1,1]}\")\n",
        "\n",
        "            # Plot precision, recall, F1-score\n",
        "            metrics = {'Precision': precision, 'Recall': recall, 'F1-Score': f1}\n",
        "            plt.figure(figsize=(8, 6))\n",
        "            plt.bar(metrics.keys(), metrics.values(), color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
        "            plt.ylim(0, 1)\n",
        "            plt.xlabel('Metrics')\n",
        "            plt.ylabel('Score')\n",
        "            plt.title(f'{dataset_name} Metrics: Precision, Recall, F1-Score')\n",
        "            for i, v in enumerate(metrics.values()):\n",
        "                plt.text(i, v + 0.02, f'{v:.4f}', ha='center')\n",
        "            plt.savefig(f'{dataset_name.lower()}_metrics.png')\n",
        "            plt.close()\n",
        "\n",
        "            # Plot confusion matrix heatmap\n",
        "            plt.figure(figsize=(8, 6))\n",
        "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                        xticklabels=['Normal', 'Non-Normal'],\n",
        "                        yticklabels=['Normal', 'Non-Normal'])\n",
        "            plt.xlabel('Predicted')\n",
        "            plt.ylabel('True')\n",
        "            plt.title(f'{dataset_name} Confusion Matrix')\n",
        "            plt.savefig(f'{dataset_name.lower()}_confusion_matrix.png')\n",
        "            plt.close()\n",
        "\n",
        "            # Matplotlib fallback for confusion matrix if seaborn is unavailable:\n",
        "            \"\"\"\n",
        "            plt.figure(figsize=(8, 6))\n",
        "            plt.imshow(cm, interpolation='nearest', cmap='Blues')\n",
        "            plt.colorbar()\n",
        "            plt.xticks([0, 1], ['Normal', 'Non-Normal'])\n",
        "            plt.yticks([0, 1], ['Normal', 'Non-Normal'])\n",
        "            for i in range(cm.shape[0]):\n",
        "                for j in range(cm.shape[1]):\n",
        "                    plt.text(j, i, cm[i, j], ha='center', va='center')\n",
        "            plt.xlabel('Predicted')\n",
        "            plt.ylabel('True')\n",
        "            plt.title(f'{dataset_name} Confusion Matrix')\n",
        "            plt.savefig(f'{dataset_name.lower()}_confusion_matrix.png')\n",
        "            plt.close()\n",
        "            \"\"\"\n",
        "\n",
        "    # Compute metrics for validation and test sets\n",
        "    compute_metrics(X_val, y_val, 'Validation')\n",
        "    compute_metrics(X_test, y_test, 'Test')\n",
        "\n",
        "def plot_metrics(history):\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.plot(history['train_loss'], label='Train Loss', color='#1f77b4')\n",
        "    plt.plot(history['val_loss'], label='Validation Loss', color='#ff7f0e')\n",
        "    plt.plot(history['test_loss'], label='Test Loss', color='#2ca02c')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training, Validation, and Test Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.plot(history['train_acc'], label='Train Accuracy', color='#1f77b4')\n",
        "    plt.plot(history['val_acc'], label='Validation Accuracy', color='#ff7f0e')\n",
        "    plt.plot(history['test_acc'], label='Test Accuracy', color='#2ca02c')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Training, Validation, and Test Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('metrics.png')\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pomxx2IheY9",
        "outputId": "bb64cc36-b377-4f99-d6d9-4c21994f57b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Record 101: Total annotations: 1874\n",
            "Extracted 1872 valid beats\n",
            "Warning: Number of labels (1862) does not match number of beats (1872) for record 101.\n",
            "After filtering for labels: 1862 beats, 1862 labels.\n",
            "Record 106: Total annotations: 2098\n",
            "Extracted 2097 valid beats\n",
            "Warning: Number of labels (2027) does not match number of beats (2097) for record 106.\n",
            "After filtering for labels: 2027 beats, 2027 labels.\n",
            "Record 108: Total annotations: 1824\n",
            "Extracted 1822 valid beats\n",
            "Warning: Number of labels (1762) does not match number of beats (1822) for record 108.\n",
            "After filtering for labels: 1762 beats, 1762 labels.\n",
            "Record 109: Total annotations: 2535\n",
            "Extracted 2532 valid beats\n",
            "Warning: Number of labels (2530) does not match number of beats (2532) for record 109.\n",
            "After filtering for labels: 2530 beats, 2530 labels.\n",
            "Record 112: Total annotations: 2550\n",
            "Extracted 2547 valid beats\n",
            "Warning: Number of labels (2537) does not match number of beats (2547) for record 112.\n",
            "After filtering for labels: 2537 beats, 2537 labels.\n",
            "Record 114: Total annotations: 1890\n",
            "Extracted 1889 valid beats\n",
            "Warning: Number of labels (1879) does not match number of beats (1889) for record 114.\n",
            "After filtering for labels: 1879 beats, 1879 labels.\n",
            "Record 115: Total annotations: 1962\n",
            "Extracted 1960 valid beats\n",
            "Warning: Number of labels (1952) does not match number of beats (1960) for record 115.\n",
            "After filtering for labels: 1952 beats, 1952 labels.\n",
            "Record 116: Total annotations: 2421\n",
            "Extracted 2420 valid beats\n",
            "Warning: Number of labels (2411) does not match number of beats (2420) for record 116.\n",
            "After filtering for labels: 2411 beats, 2411 labels.\n",
            "Record 118: Total annotations: 2301\n",
            "Extracted 2299 valid beats\n",
            "Warning: Number of labels (2277) does not match number of beats (2299) for record 118.\n",
            "After filtering for labels: 2277 beats, 2277 labels.\n",
            "Record 119: Total annotations: 2094\n",
            "Extracted 2093 valid beats\n",
            "Warning: Number of labels (1987) does not match number of beats (2093) for record 119.\n",
            "After filtering for labels: 1987 beats, 1987 labels.\n",
            "Record 122: Total annotations: 2479\n",
            "Extracted 2476 valid beats\n",
            "Warning: Number of labels (2474) does not match number of beats (2476) for record 122.\n",
            "After filtering for labels: 2474 beats, 2474 labels.\n",
            "Record 124: Total annotations: 1634\n",
            "Extracted 1632 valid beats\n",
            "Warning: Number of labels (1618) does not match number of beats (1632) for record 124.\n",
            "After filtering for labels: 1618 beats, 1618 labels.\n",
            "Record 201: Total annotations: 2039\n",
            "Extracted 2038 valid beats\n",
            "Warning: Number of labels (1963) does not match number of beats (2038) for record 201.\n",
            "After filtering for labels: 1963 beats, 1963 labels.\n",
            "Record 203: Total annotations: 3108\n",
            "Extracted 3105 valid beats\n",
            "Warning: Number of labels (2975) does not match number of beats (3105) for record 203.\n",
            "After filtering for labels: 2975 beats, 2975 labels.\n",
            "Record 205: Total annotations: 2672\n",
            "Extracted 2671 valid beats\n",
            "Warning: Number of labels (2656) does not match number of beats (2671) for record 205.\n",
            "After filtering for labels: 2656 beats, 2656 labels.\n",
            "Loaded 15 records: total samples = 32910\n",
            "Balanced training set. Original: 32910, Balanced: 61308\n",
            "Record 207: Total annotations: 2385\n",
            "Extracted 2383 valid beats\n",
            "Warning: Number of labels (1859) does not match number of beats (2383) for record 207.\n",
            "After filtering for labels: 1859 beats, 1859 labels.\n",
            "Record 208: Total annotations: 3040\n",
            "Extracted 3037 valid beats\n",
            "Warning: Number of labels (2951) does not match number of beats (3037) for record 208.\n",
            "After filtering for labels: 2951 beats, 2951 labels.\n",
            "Record 209: Total annotations: 3052\n",
            "Extracted 3051 valid beats\n",
            "Warning: Number of labels (3005) does not match number of beats (3051) for record 209.\n",
            "After filtering for labels: 3005 beats, 3005 labels.\n",
            "Record 215: Total annotations: 3400\n",
            "Extracted 3398 valid beats\n",
            "Warning: Number of labels (3362) does not match number of beats (3398) for record 215.\n",
            "After filtering for labels: 3362 beats, 3362 labels.\n",
            "Record 220: Total annotations: 2069\n",
            "Extracted 2066 valid beats\n",
            "Warning: Number of labels (2046) does not match number of beats (2066) for record 220.\n",
            "After filtering for labels: 2046 beats, 2046 labels.\n",
            "Loaded 5 records: total samples = 13223\n",
            "Record 100: Total annotations: 2274\n",
            "Extracted 2271 valid beats\n",
            "Record 103: Total annotations: 2091\n",
            "Extracted 2090 valid beats\n",
            "Warning: Number of labels (2084) does not match number of beats (2090) for record 103.\n",
            "After filtering for labels: 2084 beats, 2084 labels.\n",
            "Record 105: Total annotations: 2691\n",
            "Extracted 2690 valid beats\n",
            "Warning: Number of labels (2567) does not match number of beats (2690) for record 105.\n",
            "After filtering for labels: 2567 beats, 2567 labels.\n",
            "Record 111: Total annotations: 2133\n",
            "Extracted 2132 valid beats\n",
            "Warning: Number of labels (2124) does not match number of beats (2132) for record 111.\n",
            "After filtering for labels: 2124 beats, 2124 labels.\n",
            "Record 113: Total annotations: 1796\n",
            "Extracted 1794 valid beats\n",
            "Record 117: Total annotations: 1539\n",
            "Extracted 1537 valid beats\n",
            "Warning: Number of labels (1534) does not match number of beats (1537) for record 117.\n",
            "After filtering for labels: 1534 beats, 1534 labels.\n",
            "Record 121: Total annotations: 1876\n",
            "Extracted 1874 valid beats\n",
            "Warning: Number of labels (1862) does not match number of beats (1874) for record 121.\n",
            "After filtering for labels: 1862 beats, 1862 labels.\n",
            "Record 123: Total annotations: 1519\n",
            "Extracted 1517 valid beats\n",
            "Record 200: Total annotations: 2792\n",
            "Extracted 2790 valid beats\n",
            "Warning: Number of labels (2600) does not match number of beats (2790) for record 200.\n",
            "After filtering for labels: 2600 beats, 2600 labels.\n",
            "Record 202: Total annotations: 2146\n",
            "Extracted 2145 valid beats\n",
            "Warning: Number of labels (2135) does not match number of beats (2145) for record 202.\n",
            "After filtering for labels: 2135 beats, 2135 labels.\n",
            "Record 210: Total annotations: 2685\n",
            "Extracted 2682 valid beats\n",
            "Warning: Number of labels (2648) does not match number of beats (2682) for record 210.\n",
            "After filtering for labels: 2648 beats, 2648 labels.\n",
            "Record 212: Total annotations: 2763\n",
            "Extracted 2762 valid beats\n",
            "Warning: Number of labels (2747) does not match number of beats (2762) for record 212.\n",
            "After filtering for labels: 2747 beats, 2747 labels.\n",
            "Record 213: Total annotations: 3294\n",
            "Extracted 3291 valid beats\n",
            "Warning: Number of labels (3249) does not match number of beats (3291) for record 213.\n",
            "After filtering for labels: 3249 beats, 3249 labels.\n",
            "Record 214: Total annotations: 2297\n",
            "Extracted 2294 valid beats\n",
            "Warning: Number of labels (2258) does not match number of beats (2294) for record 214.\n",
            "After filtering for labels: 2258 beats, 2258 labels.\n",
            "Record 219: Total annotations: 2312\n",
            "Extracted 2312 valid beats\n",
            "Warning: Number of labels (2154) does not match number of beats (2312) for record 219.\n",
            "After filtering for labels: 2154 beats, 2154 labels.\n",
            "Record 221: Total annotations: 2462\n",
            "Extracted 2461 valid beats\n",
            "Warning: Number of labels (2427) does not match number of beats (2461) for record 221.\n",
            "After filtering for labels: 2427 beats, 2427 labels.\n",
            "Record 222: Total annotations: 2634\n",
            "Extracted 2632 valid beats\n",
            "Warning: Number of labels (2482) does not match number of beats (2632) for record 222.\n",
            "After filtering for labels: 2482 beats, 2482 labels.\n",
            "Record 228: Total annotations: 2141\n",
            "Extracted 2140 valid beats\n",
            "Warning: Number of labels (2053) does not match number of beats (2140) for record 228.\n",
            "After filtering for labels: 2053 beats, 2053 labels.\n",
            "Record 231: Total annotations: 2011\n",
            "Extracted 2010 valid beats\n",
            "Warning: Number of labels (1571) does not match number of beats (2010) for record 231.\n",
            "After filtering for labels: 1571 beats, 1571 labels.\n",
            "Record 232: Total annotations: 1816\n",
            "Extracted 1815 valid beats\n",
            "Warning: Number of labels (1780) does not match number of beats (1815) for record 232.\n",
            "After filtering for labels: 1780 beats, 1780 labels.\n",
            "Record 233: Total annotations: 3152\n",
            "Extracted 3149 valid beats\n",
            "Warning: Number of labels (3077) does not match number of beats (3149) for record 233.\n",
            "After filtering for labels: 3077 beats, 3077 labels.\n",
            "Record 234: Total annotations: 2764\n",
            "Extracted 2763 valid beats\n",
            "Warning: Number of labels (2753) does not match number of beats (2763) for record 234.\n",
            "After filtering for labels: 2753 beats, 2753 labels.\n",
            "Loaded 22 records: total samples = 49687\n",
            "Training samples: 61308, Validation samples: 13223, Test samples: 49687\n",
            "Using device: cuda\n",
            "Training phase...\n",
            "Epoch 1/10, Train Loss: 0.2205, Train Acc: 0.9075\n",
            "Epoch 2/10, Train Loss: 0.1287, Train Acc: 0.9509\n",
            "Epoch 3/10, Train Loss: 0.1154, Train Acc: 0.9550\n",
            "Epoch 4/10, Train Loss: 0.1063, Train Acc: 0.9579\n",
            "Epoch 5/10, Train Loss: 0.1011, Train Acc: 0.9589\n",
            "Epoch 6/10, Train Loss: 0.0890, Train Acc: 0.9648\n",
            "Epoch 7/10, Train Loss: 0.0777, Train Acc: 0.9682\n",
            "Epoch 8/10, Train Loss: 0.0734, Train Acc: 0.9707\n",
            "Epoch 9/10, Train Loss: 0.0707, Train Acc: 0.9716\n",
            "Epoch 10/10, Train Loss: 0.0680, Train Acc: 0.9726\n",
            "\n",
            "Evaluating saved models on validation set...\n",
            "Validation Epoch 1/10, Val Loss: 0.7945, Val Acc: 0.8055\n",
            "Validation Epoch 2/10, Val Loss: 0.6655, Val Acc: 0.8108\n",
            "Validation Epoch 3/10, Val Loss: 1.3355, Val Acc: 0.8072\n",
            "Validation Epoch 4/10, Val Loss: 0.8490, Val Acc: 0.8093\n",
            "Validation Epoch 5/10, Val Loss: 1.0283, Val Acc: 0.8105\n",
            "Validation Epoch 6/10, Val Loss: 1.1397, Val Acc: 0.7789\n",
            "Validation Epoch 7/10, Val Loss: 0.8606, Val Acc: 0.7938\n",
            "Validation Epoch 8/10, Val Loss: 0.8171, Val Acc: 0.7938\n",
            "Validation Epoch 9/10, Val Loss: 1.6292, Val Acc: 0.7980\n",
            "Validation Epoch 10/10, Val Loss: 0.7031, Val Acc: 0.8352\n",
            "\n",
            "Evaluating saved models on test set...\n",
            "Test Epoch 1/10, Test Loss: 0.4584, Test Acc: 0.8702\n",
            "Test Epoch 2/10, Test Loss: 0.5392, Test Acc: 0.8776\n",
            "Test Epoch 3/10, Test Loss: 0.5335, Test Acc: 0.8822\n",
            "Test Epoch 4/10, Test Loss: 0.5099, Test Acc: 0.8825\n",
            "Test Epoch 5/10, Test Loss: 0.5209, Test Acc: 0.8799\n",
            "Test Epoch 6/10, Test Loss: 0.6803, Test Acc: 0.8434\n",
            "Test Epoch 7/10, Test Loss: 0.7197, Test Acc: 0.8125\n",
            "Test Epoch 8/10, Test Loss: 0.6189, Test Acc: 0.8587\n",
            "Test Epoch 9/10, Test Loss: 0.8325, Test Acc: 0.7631\n",
            "Test Epoch 10/10, Test Loss: 0.6382, Test Acc: 0.8582\n",
            "\n",
            "Validation Metrics:\n",
            "  Loss: 0.7031\n",
            "  Accuracy: 0.8352\n",
            "  Precision: 0.5343\n",
            "  Recall: 0.5011\n",
            "  F1-Score: 0.5172\n",
            "  Confusion Matrix:\n",
            "    True Negative (Normal correct): 9877\n",
            "    False Positive (Normal as Non-Normal): 1017\n",
            "    False Negative (Non-Normal as Normal): 1162\n",
            "    True Positive (Non-Normal correct): 1167\n",
            "\n",
            "Test Metrics:\n",
            "  Loss: 0.6382\n",
            "  Accuracy: 0.8582\n",
            "  Precision: 0.3788\n",
            "  Recall: 0.4593\n",
            "  F1-Score: 0.4152\n",
            "  Confusion Matrix:\n",
            "    True Negative (Normal correct): 40140\n",
            "    False Positive (Normal as Non-Normal): 4102\n",
            "    False Negative (Non-Normal as Normal): 2944\n",
            "    True Positive (Non-Normal correct): 2501\n"
          ]
        }
      ],
      "source": [
        "def process_record(record_id, data_dir):\n",
        "    signal, rpeaks, fs, ann = load_ecg(record_id, data_dir)\n",
        "    print(f\"Record {record_id}: Total annotations: {len(ann.sample)}\")\n",
        "\n",
        "    signal_filtered = bandpass_filter(signal, fs)\n",
        "    signal_filtered = notch_filter(signal_filtered, fs)\n",
        "    signal_filtered = remove_baseline(signal_filtered, fs)\n",
        "\n",
        "    beats, valid_rpeaks = extract_heartbeats(signal_filtered, fs, ann.sample)\n",
        "    print(f\"Extracted {len(beats)} valid beats\")\n",
        "\n",
        "    if len(beats) == 0:\n",
        "        print(f\"No valid beats extracted for record {record_id}.\")\n",
        "        return np.array([]), np.array([])\n",
        "\n",
        "    beats = normalize_beats(beats)\n",
        "    labels = create_labels(valid_rpeaks, ann)\n",
        "\n",
        "    if len(labels) != len(beats):\n",
        "        print(f\"Warning: Number of labels ({len(labels)}) does not match number of beats ({len(beats)}) for record {record_id}.\")\n",
        "        labeled_beats = []\n",
        "        labeled_valid_rpeaks = []\n",
        "        labeled_labels = []\n",
        "        for i, rpeak in enumerate(valid_rpeaks):\n",
        "            idx = np.where(ann.sample == rpeak)[0]\n",
        "            if len(idx) > 0:\n",
        "                symbol = ann.symbol[idx[0]]\n",
        "                label = get_class_from_symbol(symbol)\n",
        "                if label is not None:\n",
        "                    labeled_beats.append(beats[i])\n",
        "                    labeled_valid_rpeaks.append(rpeak)\n",
        "                    labeled_labels.append(label)\n",
        "        beats = np.array(labeled_beats)\n",
        "        valid_rpeaks = np.array(labeled_valid_rpeaks)\n",
        "        labels = np.array(labeled_labels)\n",
        "        print(f\"After filtering for labels: {len(beats)} beats, {len(labels)} labels.\")\n",
        "\n",
        "    if len(beats) == 0:\n",
        "        print(f\"No beats with valid labels extracted for record {record_id}.\")\n",
        "        return np.array([]), np.array([])\n",
        "\n",
        "    beats = beats.reshape(-1, beats.shape[1], 1)\n",
        "    return beats, labels\n",
        "\n",
        "def load_dataset(record_ids, data_dir, balance_training=False):\n",
        "    all_beats = []\n",
        "    all_labels = []\n",
        "    for record_id in record_ids:\n",
        "        X, y = process_record(str(record_id), data_dir)\n",
        "        if X.shape[0] > 0:\n",
        "            all_beats.append(X)\n",
        "            all_labels.append(y)\n",
        "        else:\n",
        "            print(f\"Skipping record {record_id} due to processing issues or no valid beats.\")\n",
        "\n",
        "    if all_beats:\n",
        "        X_all = np.concatenate(all_beats, axis=0)\n",
        "        y_all = np.concatenate(all_labels, axis=0)\n",
        "        print(f\"Loaded {len(record_ids)} records: total samples = {X_all.shape[0]}\")\n",
        "\n",
        "        if balance_training:\n",
        "            beats_flat = X_all.reshape(X_all.shape[0], -1)\n",
        "            unique_classes = np.unique(y_all)\n",
        "            if len(unique_classes) > 1:\n",
        "                try:\n",
        "                    smote = SMOTE(random_state=42)\n",
        "                    X_balanced, y_balanced = smote.fit_resample(beats_flat, y_all)\n",
        "                    print(f\"Balanced training set. Original: {len(X_all)}, Balanced: {len(X_balanced)}\")\n",
        "                    X_balanced = X_balanced.reshape(-1, X_all.shape[1], 1)\n",
        "                    return X_balanced, y_balanced\n",
        "                except ValueError as e:\n",
        "                    print(f\"Could not balance training set due to error: {e}\")\n",
        "                    print(\"Using original unbalanced training data.\")\n",
        "                    return X_all, y_all\n",
        "            else:\n",
        "                print(f\"Only one class ({unique_classes[0]}) in training set, skipping balancing.\")\n",
        "                return X_all, y_all\n",
        "        else:\n",
        "            return X_all, y_all\n",
        "    else:\n",
        "        print(f\"No valid data loaded from {len(record_ids)} records.\")\n",
        "        return np.array([]), np.array([])\n",
        "\n",
        "\n",
        "# Define datasets\n",
        "DS1_train = [101, 106, 108, 109, 112, 114, 115, 116, 118, 119, 122, 124, 201, 203, 205]\n",
        "DS1_val = [207, 208, 209, 215, 220]\n",
        "DS2 = [100, 103, 105, 111, 113, 117, 121, 123, 200, 202, 210, 212, 213, 214, 219, 221, 222, 228, 231, 232, 233, 234]\n",
        "\n",
        "# Run the pipeline\n",
        "data_dir = '/content/drive/MyDrive/ecg_snn_project/data/mitdb'\n",
        "\n",
        "if not os.path.exists(os.path.join(data_dir, '100.dat')):\n",
        "    print(f\"Data directory or files not found. Please ensure {data_dir} is correct and contains MIT-BIH files.\")\n",
        "else:\n",
        "    X_train, y_train = load_dataset(DS1_train, data_dir, balance_training=True)\n",
        "    X_val, y_val = load_dataset(DS1_val, data_dir, balance_training=False)\n",
        "    X_test, y_test = load_dataset(DS2, data_dir, balance_training=False)\n",
        "\n",
        "    if X_train.shape[0] > 0 and X_val.shape[0] > 0 and X_test.shape[0] > 0:\n",
        "        print(f\"Training samples: {X_train.shape[0]}, Validation samples: {X_val.shape[0]}, Test samples: {X_test.shape[0]}\")\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        print(f\"Using device: {device}\")\n",
        "        model, history = train_model(X_train, y_train, X_val, y_val, X_test, y_test, batch_size=64, num_epochs=10, device=device)\n",
        "        evaluate_model(model, X_val, y_val, X_test, y_test, device=device)\n",
        "    else:\n",
        "        print(\"Insufficient data loaded for training, validation, or testing. Cannot proceed with model training.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "0QlwUv0Lhov-"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPG7I8WMmz7thtFjJ6ZjmIL",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}